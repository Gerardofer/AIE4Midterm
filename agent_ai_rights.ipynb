{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.1 which is incompatible.\n",
      "langchain-qdrant 0.1.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langgraph langchain langchain_openai langchain_experimental pymupdf tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from helper.file_loader import FileLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(api_key=open_ai_key, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader1 = PyMuPDFLoader(\"https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\")\n",
    "document1 = loader1.load()\n",
    "\n",
    "loader2 = PyMuPDFLoader(\"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\")\n",
    "document2 = loader2.load()\n",
    "\n",
    "document = document1 + document2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o-mini\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    split_chunks,\n",
    "    embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"extending_context_window_llama_3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{question}\n",
    "\n",
    "You are an expert of 30 years of experience that can answer only questions related to the context provided. You will submit comprehensive responses and if you can't answer the question, say you don't have enough context at the moment to answer the question.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | openai_chat_model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses several aspects of Human-AI Configuration, highlighting its significance in AI deployment and operational settings. Here are the key points:\n",
      "\n",
      "1. **Human-AI Configuration Tasks**: The document lists various tasks associated with Human-AI Configuration, including AI Deployment, Domain Experts, End-Users, Operation and Monitoring, and TEVV (Testing, Evaluation, Verification, and Validation) (page 42).\n",
      "\n",
      "2. **Measures and Actions**:\n",
      "   - **Monitoring and Tracking**: It suggests implementing systems to continually monitor and track the outcomes of human-GAI (Generative AI) configurations for future refinement and improvements (page 28).\n",
      "   - **Involvement in Prototyping and Testing**: It emphasizes involving end-users, practitioners, and operators in the prototyping and testing activities of GAI systems. These tests should cover various scenarios, such as crisis situations or ethically sensitive contexts (page 28).\n",
      "\n",
      "3. **Privacy and Risk Minimization**: Techniques such as anonymization and differential privacy are recommended to minimize risks associated with linking AI-generated content back to individual human subjects. This is crucial for maintaining data privacy within Human-AI Configurations (page 33).\n",
      "\n",
      "4. **Performance Measurement**:\n",
      "   - **Baseline Model Performance**: The document advises considering baseline model performance on suites of benchmarks when selecting a model for fine-tuning or enhancement with retrieval-augmented generation (page 33).\n",
      "   - **Empirical Evaluation**: It stresses the importance of evaluating claims of model capabilities using empirically validated methods (page 33).\n",
      "   - **Sharing Pre-deployment Testing Results**: Results of pre-deployment testing should be shared with relevant GAI Actors, such as those with system release approval authority (page 33).\n",
      "\n",
      "These points indicate a comprehensive approach to integrating human factors into AI systems, ensuring that configurations are monitored, tested, and refined continually for optimal performance and risk management.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke({\"question\" : \"can you tell me what the document says about Human-AI Configuration\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4-Day-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
